{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRABAJO FINAL PROCESAMIENTO DEL LENGUAJE NATURAL \n",
    "ALUMNOS : \n",
    "\n",
    "- ALEJANDRO MADRID GALARZA\n",
    "- ANTONIO JOSÉ LÓPEZ MARTÍNEZ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MOTIVACIÓN DEL TRABAJO\n",
    "Vamos a realizar una aplicacion 'python' en 'jupyter-lab' para la asignatura de Procesamiento del Lenguaje Natural en la que trataremos, con los conocimientos adquiridos en la asignatura así como todo lo que sea necesario para la resolución del mismo, una aplicación que clasifique un conjunto de más de 3500 'tweets' en un clasificador de 11 clases que corresponden a 11 emociones distintas sobre las que clasificaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 11:50:29.695432: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import unidecode\n",
    "#\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "#\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGAMOS LOS DATOS\n",
    "train_data = pd.read_csv('Data/Train/sem_eval_train_es.csv')\n",
    "test_data = pd.read_csv('Data/Test/sem_eval_test_grupo_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3561, 13), (679, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3561, 12) (679, 1)\n",
      "                                               Tweet  anger  anticipation  \\\n",
      "0  @aliciaenp Ajajjaa somos del clan twitteras pe...  False         False   \n",
      "1  @AwadaNai la mala suerte del gato fichame la c...  False         False   \n",
      "2  @audiomano A mí tampoco me agrado mucho eso. E...   True         False   \n",
      "3  Para llevar a los bebes de un lugar a otro deb...  False         False   \n",
      "4  @DalasReview me encanta la terrible hipocresia...   True         False   \n",
      "\n",
      "   disgust   fear    joy   love  optimism  pessimism  sadness  surprise  trust  \n",
      "0    False  False   True  False     False      False    False     False  False  \n",
      "1    False   True  False  False     False       True    False     False  False  \n",
      "2    False  False  False  False     False      False    False     False  False  \n",
      "3    False  False   True  False     False      False    False     False  False  \n",
      "4     True  False  False  False     False      False    False     False  False  \n",
      "                                               Tweet\n",
      "0  Siguen los amigos que quiero tener, no me pued...\n",
      "1  Baby a veces me hago la enojada solo porque qu...\n",
      "2              @CandeSolsu Callate que sigo ofendida\n",
      "3  Va a tener que pedir perdón por celebrar el dí...\n",
      "4  Domingo, 23:36   Quiero escuchar música y cant...\n"
     ]
    }
   ],
   "source": [
    "# Vamos a eliminar los id's de ambos csv\n",
    "train_data = train_data.drop(['ID'], axis=1)\n",
    "test_data = test_data.drop(['ID'], axis=1)\n",
    "print(train_data.shape, test_data.shape)\n",
    "print(train_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos eliminado la columna de ID's de los conjuntos de entrenamiento y testeo tenemos que separar nuestros tweets de testeo que, como podemos observar tiene 12 dimensiones (11 de las diferentes emociones + 1 para los tweets). \n",
    "Los vamos a separar como: \n",
    "- $X=contenido$ $tweets$\n",
    "- $Y=categoría$ $tweet$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3561,) (3561, 11) \n",
      "\n",
      "0    @aliciaenp Ajajjaa somos del clan twitteras pe...\n",
      "1    @AwadaNai la mala suerte del gato fichame la c...\n",
      "2    @audiomano A mí tampoco me agrado mucho eso. E...\n",
      "3    Para llevar a los bebes de un lugar a otro deb...\n",
      "4    @DalasReview me encanta la terrible hipocresia...\n",
      "Name: Tweet, dtype: object \n",
      "\n",
      "   anger  anticipation  disgust   fear    joy   love  optimism  pessimism  \\\n",
      "0  False         False    False  False   True  False     False      False   \n",
      "1  False         False    False   True  False  False     False       True   \n",
      "2   True         False    False  False  False  False     False      False   \n",
      "3  False         False    False  False   True  False     False      False   \n",
      "4   True         False     True  False  False  False     False      False   \n",
      "\n",
      "   sadness  surprise  trust  \n",
      "0    False     False  False  \n",
      "1    False     False  False  \n",
      "2    False     False  False  \n",
      "3    False     False  False  \n",
      "4    False     False  False  \n"
     ]
    }
   ],
   "source": [
    "X = train_data['Tweet']\n",
    "y = train_data.drop(['Tweet'], axis=1)\n",
    "print(X.shape, y.shape, '\\n')\n",
    "print(X.head(), '\\n')\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesado de datos\n",
    "Ahora solo vamos a trabajar con el conjunto de entrenamiento ya que tenemos que masticar los datos y pasárselos al modelo que posteriormente entrenamremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comenzamos el preprocesado cargando el modelo a ejecutar\n",
    "nlp=spacy.load('Data/Model/es_core_news_sm-3.7.0-py3-none-any/es_core_news_sm/es_core_news_sm-3.7.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/alejandro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alejandro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# En esta celda vamos a definir las funciones para la limpieza del texto\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "def limpiar_texto(tweet):\n",
    "    # Elimina los acentos y convierte caracteres especiales\n",
    "    tweet = unidecode.unidecode(tweet)\n",
    "    # Elimina menciones y hashtags\n",
    "    tweet = re.sub(r'@[A-Za-z0-9_]+', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # Tokeniza el tweet\n",
    "    tokens = word_tokenize(tweet, language='spanish')\n",
    "    # Elimina stop words y palabras de 1 letra, y realiza stemming\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word.lower() not in stop_words and len(word) > 1]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiamos nuestros tweets\n",
    "tweets_limpios = []\n",
    "for tweet in X: \n",
    "    tweet_limpio = limpiar_texto(tweet)\n",
    "    tweets_limpios.append(tweet_limpio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"ajajja clan twitter perd pa event `` import ''\",\n",
       "       'mal suert gat ficham car help pls',\n",
       "       'tampoc agrad especial trat justif', ...,\n",
       "       'prim anos enfad conmig explic azul sol chic ros chic ensen',\n",
       "       'jajaj ... see', 'quier abraz quier quer hac falt carin gent'],\n",
       "      dtype='<U106')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(tweets_limpios)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora que tenemos los datos limpios vamos a separar en entrenamiento y testeo para el modelo \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2848, 4978) (713, 4978) (2848,) (713,)\n"
     ]
    }
   ],
   "source": [
    "# Creamos la representación vecrotial de nuestros datos por el método TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "# Además vamos a transformar nuestros y en 1-D para los modelos\n",
    "y_train = y_train.idxmax(axis=1)\n",
    "y_test = y_test.idxmax(axis=1)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creada la BOW, en nuestro caso por el método TF-IDF tenemos que crear el modelo que usaremos para la predicció y entrenarlo con nuestros datos de entrenamiento. \n",
    "### Modelo\n",
    "En nuestro caso vamos a realizar un pequeño estudio sobre 3 modelos para saber cuál es mejor para nuestro Dataset y luego decidiremos el mejor y lo usaremospara la predicción. \n",
    "Los modelos a tratar va a ser:\n",
    "- Multinomial Naive Bayes\n",
    "- SVM\n",
    "- Transformer --> BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4544179523141655"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos el modelo Naive Bayes\n",
    "nbModelo = MultinomialNB()\n",
    "ajusteNB = nbModelo.fit(X_train, y_train)           # Entrenamos el modelo\n",
    "nBScore = nbModelo.score(X_test, y_test)            # Sacamos la precisión del modelo\n",
    "nBScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como esta precisión es bastante mala vamos a probar con la validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntuaciones: [0.45789474 0.48421053 0.46140351 0.49033392 0.4745167 ]\n",
      "\n",
      "Puntuación media: 0.47367187740881195\n",
      "\n",
      "Desviación Estandar: 0.012560505625461127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Repetimos el proceso pero con validación cruzada\n",
    "nBModelo = MultinomialNB()\n",
    "scores = cross_val_score(nbModelo, X_train, y_train, cv=5)\n",
    "print(f\"Puntuaciones: {scores}\\n\")\n",
    "print(f'Puntuación media: {scores.mean()}\\n')\n",
    "print(f'Desviación Estandar: {scores.std()}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la validación cruzada el número aumenta pero no es nada sustancial como para que podemos usar este modelo para nuesto dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Máquina Vector Soporte (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.52      0.84      0.64       263\n",
      "anticipation       0.43      0.11      0.18        79\n",
      "     disgust       0.00      0.00      0.00        27\n",
      "        fear       0.76      0.45      0.56        49\n",
      "         joy       0.56      0.66      0.61       166\n",
      "        love       0.00      0.00      0.00        15\n",
      "    optimism       0.00      0.00      0.00        16\n",
      "   pessimism       0.38      0.07      0.11        45\n",
      "     sadness       0.63      0.45      0.53        42\n",
      "    surprise       0.00      0.00      0.00         9\n",
      "       trust       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.54       713\n",
      "   macro avg       0.30      0.23      0.24       713\n",
      "weighted avg       0.48      0.54      0.47       713\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandro/Documents/Repositorios/PLN-FinalPr/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/alejandro/Documents/Repositorios/PLN-FinalPr/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/alejandro/Documents/Repositorios/PLN-FinalPr/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Creamos el modelo y lo aplicamos a nuestros datos\n",
    "modeloSVC = SVC(kernel='linear')\n",
    "modeloSVC.fit(X_train, y_train)\n",
    "y_predicciones = modeloSVC.predict(X_test) # Predecimos el conjunto de testeo del entrenamiento\n",
    "print(f'SVM: Classification Report: {classification_report(y_test, y_predicciones)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, estamos teniendo problemas con este modelo ya que tenemos precisiones de 0% en algunas de las categorías así como que la media de accuracy de nuestro modelo es de 53%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes Recurrentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como estamos teniendo muchos problemas con los modelos previos, vamos a pasarnos al ámbito de las redes neuronales, concretamente en las redes neuronales recurrentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Long-Short Term Memory (LSTM)\n",
    "Esta va a ser la primera red recurrente que vamos a probar ya que es eficaz a la hora de capturar las dependencias a largo plao en datos secuenciales.\n",
    "\n",
    "Para este modelo necestamos sacar los datos de tipo texto otra vez, ya que este modelo se encarga de hacer todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3561,) (3561, 11) \n",
      "\n",
      "0    @aliciaenp Ajajjaa somos del clan twitteras pe...\n",
      "1    @AwadaNai la mala suerte del gato fichame la c...\n",
      "2    @audiomano A mí tampoco me agrado mucho eso. E...\n",
      "3    Para llevar a los bebes de un lugar a otro deb...\n",
      "4    @DalasReview me encanta la terrible hipocresia...\n",
      "Name: Tweet, dtype: object \n",
      "\n",
      "   anger  anticipation  disgust   fear    joy   love  optimism  pessimism  \\\n",
      "0  False         False    False  False   True  False     False      False   \n",
      "1  False         False    False   True  False  False     False       True   \n",
      "2   True         False    False  False  False  False     False      False   \n",
      "3  False         False    False  False   True  False     False      False   \n",
      "4   True         False     True  False  False  False     False      False   \n",
      "\n",
      "   sadness  surprise  trust  \n",
      "0    False     False  False  \n",
      "1    False     False  False  \n",
      "2    False     False  False  \n",
      "3    False     False  False  \n",
      "4    False     False  False  \n"
     ]
    }
   ],
   "source": [
    "X = train_data['Tweet']\n",
    "y = train_data.drop(['Tweet'], axis=1)\n",
    "print(X.shape, y.shape, '\\n')\n",
    "print(X.head(), '\\n')\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2848,) (2848, 11) (713,) (713, 11)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2848, 1000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos a tokenizar\n",
    "max_words = 1000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=1000)\n",
    "X_train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandro/Documents/Repositorios/PLN-FinalPr/.venv/lib/python3.9/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Definimos el modelo LSTM\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(input_dim=max_words, output_dim=100, input_length=100))\n",
    "model_lstm.add(LSTM(units=64))\n",
    "model_lstm.add(Dense(units=11, activation='softmax'))  # Ajusta num_classes según tu problema\n",
    "model_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 417ms/step - accuracy: 0.2589 - loss: 3.9110 - val_accuracy: 0.3719 - val_loss: 4.0219\n",
      "Epoch 2/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 370ms/step - accuracy: 0.3454 - loss: 3.9004 - val_accuracy: 0.4105 - val_loss: 3.9430\n",
      "Epoch 3/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 363ms/step - accuracy: 0.3286 - loss: 3.7501 - val_accuracy: 0.4105 - val_loss: 3.9235\n",
      "Epoch 4/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 356ms/step - accuracy: 0.3486 - loss: 3.7788 - val_accuracy: 0.2263 - val_loss: 3.8988\n",
      "Epoch 5/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 367ms/step - accuracy: 0.2762 - loss: 3.8049 - val_accuracy: 0.4105 - val_loss: 3.8949\n",
      "Epoch 6/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 355ms/step - accuracy: 0.2989 - loss: 3.7220 - val_accuracy: 0.4105 - val_loss: 3.9017\n",
      "Epoch 7/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 347ms/step - accuracy: 0.3307 - loss: 3.7878 - val_accuracy: 0.4105 - val_loss: 3.8964\n",
      "Epoch 8/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 352ms/step - accuracy: 0.2922 - loss: 3.7696 - val_accuracy: 0.2263 - val_loss: 3.9124\n",
      "Epoch 9/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 357ms/step - accuracy: 0.2846 - loss: 3.7688 - val_accuracy: 0.2263 - val_loss: 3.8981\n",
      "Epoch 10/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 352ms/step - accuracy: 0.2668 - loss: 3.8139 - val_accuracy: 0.4105 - val_loss: 3.8906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x14f133e80>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.fit(X_train_padded, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Red Neuronal Recurrente Convolucional (CRNN) \n",
    "Esta va a ser la segunda red neuronal que vamos a probar. Estas aplican ambos tipos de redes neuronales para capturar información tanto secuencial como local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
