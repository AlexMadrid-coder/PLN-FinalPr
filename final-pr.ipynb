{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRABAJO FINAL PROCESAMIENTO DEL LENGUAJE NATURAL \n",
    "ALUMNOS : \n",
    "\n",
    "- ALEJANDRO MADRID GALARZA\n",
    "- ANTONIO JOSÉ LÓPEZ MARTÍNEZ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MOTIVACIÓN DEL TRABAJO\n",
    "Vamos a realizar una aplicacion 'python' en 'jupyter-lab' para la asignatura de Procesamiento del Lenguaje Natural en la que trataremos, con los conocimientos adquiridos en la asignatura así como todo lo que sea necesario para la resolución del mismo, una aplicación que clasifique un conjunto de más de 3500 'tweets' en un clasificador de 11 clases que corresponden a 11 emociones distintas sobre las que clasificaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "#\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "#\n",
    "from unidecode import unidecode\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGAMOS LOS DATOS\n",
    "train_data = pd.read_csv('Data/Train/sem_eval_train_es.csv')\n",
    "test_data = pd.read_csv('Data/Test/sem_eval_test_grupo_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3561, 13), (679, 2))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3561, 12) (679, 1)\n",
      "                                               Tweet  anger  anticipation  \\\n",
      "0  @aliciaenp Ajajjaa somos del clan twitteras pe...  False         False   \n",
      "1  @AwadaNai la mala suerte del gato fichame la c...  False         False   \n",
      "2  @audiomano A mí tampoco me agrado mucho eso. E...   True         False   \n",
      "3  Para llevar a los bebes de un lugar a otro deb...  False         False   \n",
      "4  @DalasReview me encanta la terrible hipocresia...   True         False   \n",
      "\n",
      "   disgust   fear    joy   love  optimism  pessimism  sadness  surprise  trust  \n",
      "0    False  False   True  False     False      False    False     False  False  \n",
      "1    False   True  False  False     False       True    False     False  False  \n",
      "2    False  False  False  False     False      False    False     False  False  \n",
      "3    False  False   True  False     False      False    False     False  False  \n",
      "4     True  False  False  False     False      False    False     False  False  \n",
      "                                               Tweet\n",
      "0  Siguen los amigos que quiero tener, no me pued...\n",
      "1  Baby a veces me hago la enojada solo porque qu...\n",
      "2              @CandeSolsu Callate que sigo ofendida\n",
      "3  Va a tener que pedir perdón por celebrar el dí...\n",
      "4  Domingo, 23:36   Quiero escuchar música y cant...\n"
     ]
    }
   ],
   "source": [
    "# Vamos a eliminar los id's de ambos csv\n",
    "train_data = train_data.drop(['ID'], axis=1)\n",
    "test_data = test_data.drop(['ID'], axis=1)\n",
    "print(train_data.shape, test_data.shape)\n",
    "print(train_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos eliminado la columna de ID's de los conjuntos de entrenamiento y testeo tenemos que separar nuestros tweets de testeo que, como podemos observar tiene 12 dimensiones (11 de las diferentes emociones + 1 para los tweets). \n",
    "Los vamos a separar como: \n",
    "- $X=contenido$ $tweets$\n",
    "- $Y=categoría$ $tweet$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3561,) (3561, 11) \n",
      "\n",
      "0    @aliciaenp Ajajjaa somos del clan twitteras pe...\n",
      "1    @AwadaNai la mala suerte del gato fichame la c...\n",
      "2    @audiomano A mí tampoco me agrado mucho eso. E...\n",
      "3    Para llevar a los bebes de un lugar a otro deb...\n",
      "4    @DalasReview me encanta la terrible hipocresia...\n",
      "Name: Tweet, dtype: object \n",
      "\n",
      "   anger  anticipation  disgust   fear    joy   love  optimism  pessimism  \\\n",
      "0  False         False    False  False   True  False     False      False   \n",
      "1  False         False    False   True  False  False     False       True   \n",
      "2   True         False    False  False  False  False     False      False   \n",
      "3  False         False    False  False   True  False     False      False   \n",
      "4   True         False     True  False  False  False     False      False   \n",
      "\n",
      "   sadness  surprise  trust  \n",
      "0    False     False  False  \n",
      "1    False     False  False  \n",
      "2    False     False  False  \n",
      "3    False     False  False  \n",
      "4    False     False  False  \n"
     ]
    }
   ],
   "source": [
    "X = train_data['Tweet']\n",
    "y = train_data.drop(['Tweet'], axis=1)\n",
    "print(X.shape, y.shape, '\\n')\n",
    "print(X.head(), '\\n')\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesado de datos\n",
    "Ahora solo vamos a trabajar con el conjunto de entrenamiento ya que tenemos que masticar los datos y pasárselos al modelo que posteriormente entrenamremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comenzamos el preprocesado cargando el modelo a ejecutar\n",
    "nlp=spacy.load('Data/Model/es_core_news_sm-3.7.0-py3-none-any/es_core_news_sm/es_core_news_sm-3.7.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En esta celda vamos a definir las funciones para la limpieza del texto\n",
    "def limpiar_texto(tweet):\n",
    "    tweet = unidecode(tweet)                                                            # Elimina los acentos\n",
    "    tweet = re.sub(r'@[A-Za-z0-9_]+', '', tweet)                                        # Eliminamos mencinoes\n",
    "    doc = nlp(tweet)                                                                    # Tokenizamos el tweet\n",
    "    tokens = [token.text for token in doc if not token.is_stop and len(token.text) > 1] # Eliminamos stop words y palabras de 1 letra\n",
    "    return \" \".join(tokens)                                                             # Unimos los tokens, único string\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Ajajjaa clan twitteras perdidas pa eventos imp...\n",
      "1               mala suerte gato fichame cara help pls\n",
      "2          agrado Especialmente tratarse justificacion\n",
      "3    bebes lugar debemos cantarles canciones ... Qu...\n",
      "4    encanta terrible hipocresia doble moral gente ...\n",
      "Name: Tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Limpiamos nuestros tweets\n",
    "X = X.apply(limpiar_texto)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3561,) (3561, 11)\n",
      "(2848,) (713,) (2848, 11) (713, 11)\n"
     ]
    }
   ],
   "source": [
    "# Ahora que tenemos los datos limpios vamos a separar en entrenamiento y testeo para el modelo \n",
    "print(X.shape, y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2848, 6899) (713, 6899) (2848,) (713,)\n"
     ]
    }
   ],
   "source": [
    "# Creamos la representación vecrotial de nuestros datos por el método TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "# Además vamos a transformar nuestros y en 1-D para los modelos\n",
    "y_train = y_train.idxmax(axis=1)\n",
    "y_test = y_test.idxmax(axis=1)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creada la BOW, en nuestro caso por el método TF-IDF tenemos que crear el modelo que usaremos para la predicció y entrenarlo con nuestros datos de entrenamiento. \n",
    "### Modelo\n",
    "En nuestro caso vamos a realizar un pequeño estudio sobre 3 modelos para saber cuál es mejor para nuestro Dataset y luego decidiremos el mejor y lo usaremospara la predicción. \n",
    "Los modelos a tratar va a ser:\n",
    "- Multinomial Naive Bayes\n",
    "- SVM\n",
    "- Transformer --> BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_BaseNB.predict() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m nbModelo \u001b[38;5;241m=\u001b[39m MultinomialNB()\n\u001b[1;32m      3\u001b[0m nbModelo\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mnbModelo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: _BaseNB.predict() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# Creamos el modelo Naive Bayes\n",
    "nbModelo = MultinomialNB()\n",
    "nbModelo.fit(X_train, y_train)\n",
    "nbModelo.predict(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
